{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6e6e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn,optim\n",
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from json_tools import load_json, dump_json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc, precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86216f",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "691185d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datas = load_json(\"ps_sents_pair_with_label_origin.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1fcbbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle datas\n",
    "datas = shuffle(shuffle(datas,random_state=22),random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbe84708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21510, 2689, 2689)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test = train_test_split(datas,test_size=0.2, random_state=2022)\n",
    "test,val = train_test_split(test,test_size=0.5, random_state=2022)\n",
    "len(train),len(test),len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f991453",
   "metadata": {},
   "source": [
    "##### complaint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34b0b0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3349"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load data and set labels\n",
    "data_complaint = pd.read_csv('data/complaint1700.csv').tweet.values\n",
    "\n",
    "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv').tweet.values\n",
    "sent_pairs = []\n",
    "for i,j in zip(shuffle(data_complaint, random_state=2022), shuffle(data_non_complaint, random_state=2022)):\n",
    "    dic01 ={\"sent1\": i,\"sent2\":j,'label':0}\n",
    "    sent_pairs.append(dic01)\n",
    "for i,j in zip(shuffle(data_complaint, random_state=2022), shuffle(data_complaint, random_state=2022)):\n",
    "    dic00 ={\"sent1\": i,\"sent2\":j,'label':1}\n",
    "    sent_pairs.append(dic00)\n",
    "sent_pairs = [sents for sents in sent_pairs if len(sents[\"sent1\"])>30 and len(sents[\"sent2\"])> 30]\n",
    "dump_json(sent_pairs,\"ps_sents_pair_with_label_data_complaint.json\")\n",
    "len(sent_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02c3f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3014, 167, 168)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test = train_test_split(sent_pairs,test_size=0.1, random_state=2022)\n",
    "test,val = train_test_split(test,test_size=0.5, random_state=2022)\n",
    "len(train),len(test),len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03d998",
   "metadata": {},
   "source": [
    "# Load the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20fa789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True,cache_dir=os.path.join(os.getcwd(),\"bert-base-uncased_cache\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bb827",
   "metadata": {},
   "source": [
    "# find the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "55ae2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  113\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents1 = [d[\"sent1\"].lower() for d in datas]\n",
    "sents2 = [d[\"sent2\"].lower() for d in datas]\n",
    "sents = list(set(sents1+sents2))\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in sents]\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d4f5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a MAX_LEN you like\n",
    "MAX_LEN=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f53f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing_data2(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e80089",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95dec116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "        print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b336f",
   "metadata": {},
   "source": [
    "# Create a function to tokenize a set of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "868ce9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids= []\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        \n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing_data2(sent['sent1'].lower()),#The first sequence to be encoded\n",
    "            text_pair=text_preprocessing_data2(sent[\"sent2\"].lower()),# second sequence to be encoded\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            return_token_type_ids=True,\n",
    "#             return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        token_type_ids.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    token_type_ids =  torch.tensor(token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38f50416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert_two_sent(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids= []\n",
    "    for sent in data:\n",
    "        A = tokenizer.encode(sent['sent1'])\n",
    "        B = tokenizer.encode(sent['sent1'])[1:]\n",
    "        input_id= A + B\n",
    "        if len(input_id) > MAX_LEN:\n",
    "            input_id = input_id[:MAX_LEN]\n",
    "            train_mask = len(input_id)*[1]\n",
    "            token_type_id = train_mask\n",
    "        else: \n",
    "            train_mask = len(input_id)*[1] + (MAX_LEN - len(input_id))*[0]\n",
    "            token_type_id = train_mask\n",
    "            input_id += (MAX_LEN - len(input_id))*[0]\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(train_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    token_type_ids =  torch.tensor(token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9442d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]),\n",
       " tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert  labels to torch.Tensor\n",
    "train_labels = torch.tensor([t[\"label\"] for t in train])\n",
    "test_labels = torch.tensor([t[\"label\"] for t in test])\n",
    "val_labels = torch.tensor([t[\"label\"] for t in val])\n",
    "train_labels,test_labels,val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4cef6195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ba356bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1507, 1507)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(train_labels).item(),len(train_labels)-torch.count_nonzero(train_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6a63db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 79)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(test_labels).item(),len(test_labels)-torch.count_nonzero(test_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9a78d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 82)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(val_labels).item(),len(val_labels)-torch.count_nonzero(val_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2a233efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train_inputs, train_masks , train_token_type_ids = preprocessing_for_bert(train)\n",
    "test_inputs, test_masks, test_token_type_ids= preprocessing_for_bert(test)\n",
    "val_inputs, val_masks ,val_token_type_ids= preprocessing_for_bert(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "679cdd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_masks[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4df9b44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  5139, 22610, 16764,  6032,  3291,  1029, 14145,  2080, 12954,\n",
       "         2773, 13828,  2064, 10651,  2149,  5064,  1029,  8299,  2522,  1038,\n",
       "        16409,  2290,  2480,  2546, 24703,  2575,  2595,   102,  5139, 22610,\n",
       "        16764,  6032,  3291,  1029, 14145,  2080, 12954,  2773, 13828,  2064,\n",
       "        10651,  2149,  5064,  1029,  8299,  2522,  1038, 16409,  2290,  2480,\n",
       "         2546, 24703,  2575,  2595,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4130904e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_type_ids[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa566f8e",
   "metadata": {},
   "source": [
    "# Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0c262dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PyTorch DataLoader\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks,train_token_type_ids, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "test_data = TensorDataset(test_inputs, test_masks,test_token_type_ids, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_token_type_ids,val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66892914",
   "metadata": {},
   "source": [
    "# Create BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe2e86",
   "metadata": {},
   "source": [
    "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the [CLS] token is used as the features of the sequence to feed a classifier.\n",
    "\n",
    "The transformers library has the BertForSequenceClassification class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
    "\n",
    "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b7527dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        # bert base 768 hidden dimension\n",
    "        # bert large 1024 hidden dimension\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "        unfreeze_layers = ['layer.10','layer.11','bert.pooler','out.']\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',cache_dir=os.path.join(os.getcwd(),\"bert-base-uncased_cache\"))\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "#             for name, param in self.bert.named_parameters():\n",
    "#                 print(name,param.size())\n",
    " \n",
    "#             print(\"*\"*30)\n",
    "#             print('\\n')\n",
    " \n",
    "            for name ,param in self.bert.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                for ele in unfreeze_layers:\n",
    "                    if ele in name:\n",
    "                        param.requires_grad = True\n",
    "                        break\n",
    "#             #验证一下\n",
    "#             for name, param in self.bert.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print(name,param.size())\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27301f",
   "metadata": {},
   "source": [
    "# Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636af2a8",
   "metadata": {},
   "source": [
    "recommend following hyper-parameters\n",
    "   - Batch size: 16 or 32       \n",
    "   - Learning rate (Adam): 5e-5, 3e-5 or 2e-5  \n",
    "   - Number of epochs: 2, 3, 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55f8541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, scheduler: str, warmup_steps: int, t_total: int):\n",
    "        \"\"\"\n",
    "        Returns the correct learning rate scheduler\n",
    "        \"\"\"\n",
    "        scheduler = scheduler.lower()\n",
    "        if scheduler == 'constantlr':\n",
    "            return transformers.get_constant_schedule(optimizer)\n",
    "        elif scheduler == 'warmupconstant':\n",
    "            return transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "        elif scheduler == 'warmuplinear':\n",
    "            return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "        elif scheduler == 'warmupcosine':\n",
    "            return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "        elif scheduler == 'warmupcosinewithhardrestarts':\n",
    "            return transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown scheduler {}\".format(scheduler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "79c2b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_train_step:  189\n"
     ]
    }
   ],
   "source": [
    "len_train_step = len(train_dataloader)\n",
    "print('len_train_step: ',len_train_step)\n",
    "\n",
    "def initialize_model(epochs=4,fp16=False,n_gpu=0,local_rank=-1):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    # bert_layers越小，学习率应该要越大\n",
    "    #optimizer = AdamW(bert_classifier.parameters(),\n",
    "    #                  lr=5e-5,    # Default learning rate\n",
    "    #                 eps=1e-8    # Default epsilon value\n",
    "    #                  )\n",
    "    # 定义优化器\n",
    "    optimizer = optim.Adam(\n",
    "                            bert_classifier.parameters(),\n",
    "                            lr=5e-5,\n",
    "                            betas=(0.9, 0.999),\n",
    "                            eps=1e-08,\n",
    "                            weight_decay=0,\n",
    "                            amsgrad=False)# Adam梯度下降\n",
    "    # Total number of training steps\n",
    "    total_steps = len_train_step * epochs\n",
    "    warm_up_ratio = 0.0 # 定义要预热的step\n",
    "    # Set up the learning rate scheduler\n",
    "    # https://blog.csdn.net/orangerfun/article/details/120400247\n",
    "    #scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "    #                                           num_warmup_steps=warm_up_ratio*total_steps, # Default value\n",
    "    #                                            num_training_steps=total_steps)\n",
    "    scheduler = get_scheduler(optimizer, 'warmuplinear', warm_up_ratio*total_steps, total_steps)\n",
    "    #### Optional configuration\n",
    "#     if fp16:\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"\n",
    "#         try:\n",
    "#             from apex import amp\n",
    "#             fp16_opt_level = '01'\n",
    "#         except ImportError:\n",
    "#             raise ImportError(\"Please install apex from \"\n",
    "#             \"https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "#         bert_classifier, optimizer = amp.initialize(\n",
    "#             bert_classifier, optimizer, opt_level=fp16_opt_level)\n",
    "#         \"For fp16: Apex AMP optimization level \"\n",
    "#         \"selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "#         \"See details at https://nvidia.github.io/apex/amp.html\"\n",
    "#     # multi-gpu training (should be after apex fp16 initialization)\n",
    "#     if n_gpu > 1:\n",
    "#         bert_classifier = torch.nn.DataParallel(bert_classifier)\n",
    "        \n",
    "#      # Distributed training (should be after apex fp16 initialization)\n",
    "#     if local_rank != -1:\n",
    "#         bert_classifier = torch.nn.parallel.DistributedDataParallel(\n",
    "#             bert_classifier, \n",
    "#             device_ids=[local_rank],\n",
    "#             output_device=local_rank,\n",
    "#             find_unused_parameters=True\n",
    "#             )\n",
    "        \n",
    "    return bert_classifier, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a2c2a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3187",
   "metadata": {},
   "source": [
    "Training:\n",
    "\n",
    "- Unpack our data from the dataloader and load the data onto the GPU\n",
    "- Zero out gradients calculated in the previous pass\n",
    "- Perform a forward pass to compute logits and loss\n",
    "- Perform a backward pass to compute gradients (loss.backward())\n",
    "- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "- Update the model's parameters (optimizer.step())\n",
    "- Update the learning rate (scheduler.step())\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "- Unpack our data and load onto the GPU\n",
    "- Forward pass\n",
    "- Compute loss and accuracy rate over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f28d5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./log/adam_base_large_lr=5e-5droupout=0.0warmup=0.0batchsize=16unfreeze_bert_10_Dec_complaint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7c482b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='mean',label_smoothing=0.05)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')#https://blog.csdn.net/zyoung17/article/details/108430465\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    one_cricle_length = len(train_dataloader)\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val F1':^9} | {'Elapsed/lr':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask,b_token_type_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids,b_token_type_ids, b_attn_mask)\n",
    "#             print('logits,b_labels')\n",
    "#             print(logits)\n",
    "#             print(b_labels)\n",
    "            '''tensor([[-0.0491,  0.1918],\n",
    "                       [-0.0062,  0.1743],\n",
    "                       [-0.0856,  0.2257],\n",
    "                       [-0.0044,  0.1629]], grad_fn=<AddmmBackward>)\n",
    "                tensor([0, 1, 0, 0])\n",
    "            '''\n",
    "            #print(logits.shape)  #torch.Size([4, 2]), 4是batch_size, 2是类别\n",
    "            #print(b_labels.shape) #torch.Size([4])\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            #print(loss)\n",
    "            #print(logits.shape,b_labels.shape,loss.shape)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "#             b = 0.64\n",
    "#             flood = (loss-b).abs()+b\n",
    "#             flood.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\",L2 norm fuc\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            op_params =optimizer.state_dict()['param_groups'][0]\n",
    "            lr = op_params.get('lr')\n",
    "            \n",
    "            ########################################################################\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                writer.add_scalar('step/train_loss_step', batch_loss / batch_counts, step + epoch_i*one_cricle_length)\n",
    "                writer.add_scalar('step/train_lr_step', lr, step + epoch_i*one_cricle_length)\n",
    "\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "#                 writer.add_scalar('steploss/train_loss_step', batch_loss / batch_counts, step)\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {lr}\")\n",
    "                \n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "            ###############################################################\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_f1, val_accuracy, val_p1, val_r1 = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            writer.add_scalar('loss/train_loss', avg_train_loss, epoch_i)\n",
    "            writer.add_scalar('loss/val_loss', val_loss, epoch_i)\n",
    "            writer.add_scalar('matrix/val_f1', val_f1, epoch_i)\n",
    "            writer.add_scalar('matrix/val_p1', val_p1, epoch_i)\n",
    "            writer.add_scalar('matrix/val_r1', val_r1, epoch_i)\n",
    "            writer.add_scalar('matrix/val_accuracy', val_accuracy, epoch_i)\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_f1:^9.2f} | {time_elapsed:^15.6f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    writer.close()\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_f1 = []\n",
    "    val_p0,val_p1,val_r0,val_r1 = [],[],[],[]\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_token_type_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids,b_token_type_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        #print(preds)\n",
    "        #print(b_labels)\n",
    "        # Calculate the accuracy rate\n",
    "        # accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        #accuracy = (preds == b_labels).cpu().numpy().mean()\n",
    "        accuracy = accuracy_score(b_labels, preds,  normalize=True)\n",
    "        f1 = f1_score(b_labels, preds, average='weighted')\n",
    "        precision = precision_score(b_labels, preds, average='weighted')\n",
    "        recall = recall_score(b_labels, preds, average='weighted')\n",
    "        val_f1.append(f1)\n",
    "        val_accuracy.append(accuracy)\n",
    "        val_p1.append(precision)\n",
    "        val_r1.append(recall)\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_f1 = np.mean(val_f1)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    val_p0 = np.mean(val_p0)\n",
    "    val_p1 = np.mean(val_p1)\n",
    "    val_r0 = np.mean(val_r0)\n",
    "    val_r1 = np.mean(val_r1)\n",
    "\n",
    "    return val_loss, val_f1, val_accuracy,val_p1,val_r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dfa",
   "metadata": {},
   "source": [
    "# running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c200b9ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Elapsed/lr\n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.236495   |     -      |     -     | 4.82363315696649e-05\n",
      "   1    |   40    |   0.007860   |     -      |     -     | 4.647266313932981e-05\n",
      "   1    |   60    |   0.003752   |     -      |     -     | 4.470899470899471e-05\n",
      "   1    |   80    |   0.002663   |     -      |     -     | 4.294532627865962e-05\n",
      "   1    |   100   |   0.002123   |     -      |     -     | 4.118165784832452e-05\n",
      "   1    |   120   |   0.001783   |     -      |     -     | 3.941798941798942e-05\n",
      "   1    |   140   |   0.001520   |     -      |     -     | 3.7654320987654326e-05\n",
      "   1    |   160   |   0.001337   |     -      |     -     | 3.5890652557319226e-05\n",
      "   1    |   180   |   0.001192   |     -      |     -     | 3.412698412698413e-05\n",
      "   1    |   188   |   0.001095   |     -      |     -     | 3.342151675485009e-05\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |    -    |   0.028676   |  0.000779  |   1.00    |   768.510042   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Elapsed/lr\n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.001022   |     -      |     -     | 3.1569664902998235e-05\n",
      "   2    |   40    |   0.000935   |     -      |     -     | 2.980599647266314e-05\n",
      "   2    |   60    |   0.000861   |     -      |     -     | 2.8042328042328043e-05\n",
      "   2    |   80    |   0.000789   |     -      |     -     | 2.6278659611992943e-05\n",
      "   2    |   100   |   0.000755   |     -      |     -     | 2.4514991181657847e-05\n",
      "   2    |   120   |   0.000700   |     -      |     -     | 2.275132275132275e-05\n",
      "   2    |   140   |   0.000659   |     -      |     -     | 2.0987654320987655e-05\n",
      "   2    |   160   |   0.000666   |     -      |     -     | 1.922398589065256e-05\n",
      "   2    |   180   |   0.000597   |     -      |     -     | 1.746031746031746e-05\n",
      "   2    |   188   |   0.000584   |     -      |     -     | 1.6754850088183422e-05\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |    -    |   0.000769   |  0.000414  |   1.00    |   743.299134   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Elapsed/lr\n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.000555   |     -      |     -     | 1.490299823633157e-05\n",
      "   3    |   40    |   0.043915   |     -      |     -     | 1.3139329805996472e-05\n",
      "   3    |   60    |   0.000636   |     -      |     -     | 1.1375661375661376e-05\n",
      "   3    |   80    |   0.000577   |     -      |     -     | 9.61199294532628e-06\n",
      "   3    |   100   |   0.000543   |     -      |     -     | 7.848324514991182e-06\n",
      "   3    |   120   |   0.000522   |     -      |     -     | 6.0846560846560845e-06\n",
      "   3    |   140   |   0.000519   |     -      |     -     | 4.3209876543209875e-06\n",
      "   3    |   160   |   0.000509   |     -      |     -     | 2.5573192239858905e-06\n",
      "   3    |   180   |   0.000527   |     -      |     -     | 7.936507936507937e-07\n",
      "   3    |   188   |   0.000508   |     -      |     -     | 8.818342151675485e-08\n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.005136   |  0.000363  |   1.00    |   744.136749   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=epochs)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=epochs, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2c67fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#saving a checkpoint assuming the network class named ClassNet\n",
    "checkpoint={'modle':bert_classifier,\n",
    "             'model_state_dict':bert_classifier.state_dict(),\n",
    "             'optimize_state_dict':optimizer.state_dict(),\n",
    "             'epoch':epochs}\n",
    "torch.save(checkpoint,'checkoutpoints/checkpoint_epoch_5_lr_5e-5_dropout_0_warmupstep_0.0_seed_42_complaint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4e8580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath,optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model=checkpoint['modle']#提前网络结构\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])#加载网络权重参数\n",
    "    optimizer=optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimize_state_dict'])#加载优化器参数\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad=False\n",
    "    model.eval()\n",
    "    return model\n",
    "modle=load_checkpoint('checkoutpoints/checkpoint_epoch_5_lr_5e-5_dropout_0_warmupstep_0.0_seed_42_complaint.pkl',optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a441d75",
   "metadata": {},
   "source": [
    "# Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cab3f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        #b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        b_input_ids, b_attn_mask, b_token_type_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids,b_token_type_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fea482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc) \n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6d901fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 1.0000\n",
      "Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(modle, test_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cbfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0425ba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d49a8830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a72d8d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   1       0\n",
       "2   2       1\n",
       "3   3       1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
